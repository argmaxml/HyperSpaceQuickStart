{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![63f78014766fd30436c18a79_Hyperspace - navbar logo.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "_trhSpIUhamm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Movie Recommendation System Using Hyperspace Engine\n",
        "This notebook demonstrates the use of Hyperspace engine to create a movie recommendation system, by combining classic search (keyword matching) with vector search. The notebook first demonstrates classic and vector search, followed by hybrid search, which combines classic and vector searches, by combining word embedding with metadata filtering.\n",
        "\n",
        "Finally, we will demonstrate the use of aggregation to analyze data regarding the returned documents.\n",
        "\n",
        "# The Dataset\n",
        "The data is taken from [MovieLens Latest Datasets](https://grouplens.org/datasets/movielens/latest/) and was downloaded from [Kaggle movie recommender system dataset ](https://www.kaggle.com/code/rounakbanik/movie-recommender-systems). The data includes 40951 valid movies. The data is in SQL format (table) and will be converted to NoSQL (documents) format. The data preprocessing is given in the notebook titles \"MovieRecommendationDataPrep\", available in this repository.\n",
        "\n",
        "## The Dataset Fields\n",
        "The processed metadata includes the following fields:\n",
        "\n",
        "1.   **adult** [boolean] - states if the movie is rated 18+\n",
        "2.   **belongs_to_collection** [Keyword] - name of the collection that includes the movie. If the movie is not a part of a collection, value will be \"None\"\n",
        "3. **budget** [integer] - The budget of the movie in USD\n",
        "4. **genres** [list[Keyword]] - list of movie genres (i.e drama)\n",
        "5. **id** [integer]] - unique id per movie\n",
        "6. **original_language** [Keyword] - the original language in which the movie was produced\n",
        "7. **popularity** [float] - the popularity of the movie, formulated as an unbounded score\n",
        "8. **production_companies** [list[Keyword]] - list of production companies involved in the movie\n",
        "9. **production_countries** [list[Keyword]] - list of all countries in which the movie was filmed\n",
        "10. **rating** [float] - the movie IMDB weighted average rating  score\n",
        "11. **release_date_unix_time** [int] - the movie release date in unix time\n",
        "12. **revenue** [float] - the movie rvenue in [USD]\n",
        "13. **runtime_days** [int] - the number cinema run time days\n",
        "14. **spoken_languages** [list[Keyword]] - list of all languages spoken in the movie\n",
        "15. **title** [Keyword] - the movie title\n",
        "16. **description embedding** [list[float]] - embedding of the movie tagline and description text\n"
      ],
      "metadata": {
        "id": "mv-I3CgcYxYL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gDLKKkcq26b"
      },
      "source": [
        "# Loading the Data\n",
        "We first load the movie metadata in an SQL from from a csv file, using the pandas module. The data was previously processed in order to only include the relevant features. The data does not include vectors, and we will create them usin simple TF-IDF embedding, in the next step.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "df = pd.read_csv('Movie_Recommendation_Processed.csv')\n",
        "df[\"runtime_days\"] = df[\"runtime_days\"].astype(int)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "fCs3k081qeQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "581c3289-a57c-4abf-ef3b-c1ad63caa9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40951 entries, 0 to 40950\n",
            "Data columns (total 18 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   index                  40951 non-null  int64  \n",
            " 1   adult                  40951 non-null  bool   \n",
            " 2   belongs_to_collection  40951 non-null  object \n",
            " 3   budget                 40951 non-null  int64  \n",
            " 4   genres                 40951 non-null  object \n",
            " 5   id                     40951 non-null  int64  \n",
            " 6   overview               40951 non-null  object \n",
            " 7   popularity             40951 non-null  float64\n",
            " 8   production_companies   40951 non-null  object \n",
            " 9   production_countries   40951 non-null  object \n",
            " 10  revenue                40951 non-null  float64\n",
            " 11  runtime_days           40951 non-null  int64  \n",
            " 12  spoken_languages       40951 non-null  object \n",
            " 13  tagline                18454 non-null  object \n",
            " 14  title                  40951 non-null  object \n",
            " 15  year                   40951 non-null  int64  \n",
            " 16  rating                 40951 non-null  float64\n",
            " 17  unix_time              40951 non-null  int64  \n",
            "dtypes: bool(1), float64(3), int64(6), object(8)\n",
            "memory usage: 5.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding\n",
        "The next step is to embedd the text of movie overviews and the taglines. We use a simple TF-IDF based vectorization  (in contrast to a more sophisticated embedding using,i.e., BERT or GPT) using the SKLEARN TfidfVectorizer. The first step will be to normalize the text and then replace rare words with base tense"
      ],
      "metadata": {
        "id": "fo0Q5qmIWijs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from pprint import pprint, pformat\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stopwords = list(stopwords.words('english'))\n",
        "replacement_dict = {\"Cheated\":\"Cheat\",\"Photographs\":\"Photograph\",\"Awfully\":\"Awful\",\"Poisoner\":\"Poisoner\",\"comix\":\"comics\",\n",
        "                    \"embarrassingly\":\"embarrassing\"}\n",
        "\n",
        "def normalize_text(tagline):\n",
        "    tagline = re.sub(r'\\W', ' ', tagline)\n",
        "\n",
        "    words = nltk.word_tokenize(tagline)\n",
        "    normalized_words = [word for word in words if word.lower() not in ['be', 'is', 'are', 'am', 'was', 'were', 'been', 'being'] + stopwords]\n",
        "    normalized_words = [lemmatizer.lemmatize(word, pos='v') for word in normalized_words if len(word) > 1 and not word.isdigit()]\n",
        "\n",
        "    normalized_tagline = ' '.join(normalized_words)\n",
        "    for key in replacement_dict:\n",
        "      normalized_tagline = normalized_tagline.replace(key,replacement_dict[key])\n",
        "    return normalized_tagline\n",
        "\n",
        "df['tagline'] = df['tagline'].fillna(\"''\")\n",
        "df['tagline'] = df['tagline'].apply(normalize_text)\n",
        "df['overview'] = df['overview'].apply(normalize_text)\n",
        "replacement_dict = {\"Cheated\":\"Cheat\",\"Photographs\":\"Photograph\",\"Awfully\":\"Awful\",\"Poisoner\":\"Poisoner\",\"comix\":\"comics\",\n",
        "                    \"embarrassingly\":\"embarrassing\"}\n",
        "\n",
        "\n",
        "df[\"description text\"] = df[\"overview\"] + df[\"tagline\"]\n",
        "del(df[\"overview\"])\n",
        "del(df[\"tagline\"])\n"
      ],
      "metadata": {
        "id": "WU7L4moRWksL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9b8293-d91b-4ff5-da7f-a23cd435fd5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "embedding_vector_length = 2048\n",
        "\n",
        "def normalize_euclidean_norm(input_list):\n",
        "    euclidean_norm = math.sqrt(sum(x ** 2 for x in input_list))\n",
        "    normalized_list = [x / euclidean_norm for x in input_list]\n",
        "    return normalized_list\n",
        "\n",
        "def embedded_text(df_data, min_word_count):\n",
        "  tfidf = TfidfVectorizer(token_pattern=r'\\b\\w+\\b', stop_words=\"english\", min_df=min_word_count)\n",
        "  tfidf_matrix = tfidf.fit_transform(df_data)\n",
        "  idf_values = tfidf.idf_\n",
        "  top_terms_idx = idf_values.argsort()[::-1][:embedding_vector_length]\n",
        "  top_terms = [list(tfidf.vocabulary_.keys())[i] for i in top_terms_idx]\n",
        "  new_tfidf = TfidfVectorizer(vocabulary=top_terms)\n",
        "  new_tfidf_matrix = new_tfidf.fit_transform(df_data)\n",
        "  new_tfidf_matrix = round(100 * new_tfidf_matrix)/100\n",
        "  tfidf_matrix = new_tfidf_matrix.toarray()\n",
        "  new_col = df_data.copy()\n",
        "  return list(tfidf_matrix), new_tfidf, top_terms\n",
        "\n",
        "df[\"description embedding\"], tfidf_model, top_terms = embedded_text(df[\"description text\"], 10)\n",
        "df[\"description embedding\"] = df[\"description embedding\"].map(lambda x: list(x))\n",
        "df[\"description embedding\"] = df[\"description embedding\"].map(lambda x: normalize_euclidean_norm(x))\n",
        "df.reset_index(inplace=True)\n"
      ],
      "metadata": {
        "id": "ENvNwg_2Wn8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "embedding_vector_length = 2048\n",
        "\n",
        "def normalize_euclidean_norm(input_list):\n",
        "    euclidean_norm = math.sqrt(sum(x ** 2 for x in input_list))\n",
        "    normalized_list = [x / euclidean_norm for x in input_list]\n",
        "    return normalized_list\n",
        "\n",
        "df[\"description embedding\"] = df[\"description embedding\"].map(lambda x: normalize_euclidean_norm(x))\n"
      ],
      "metadata": {
        "id": "gP7YIlKFleTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to allow the user to include additional text, we include the function \"embed_text\" which prfoms the embedding process for given text"
      ],
      "metadata": {
        "id": "cjXcVmkImtyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_text(text):\n",
        "  text = normalize_text(text)\n",
        "  text = text.split(\" \")\n",
        "  vec = [0] * embedding_vector_length\n",
        "  for word in text:\n",
        "    for i, term in enumerate(top_terms):\n",
        "      if word == term:\n",
        "        vec[i] = vec[i]+1\n",
        "        continue\n",
        "  return normalize_euclidean_norm(vec)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZSwKsbmjwg7",
        "outputId": "b82ce9e1-606a-4a50-fb40-414f450bd71f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.sum(np.array(df[\"description embedding\"][45]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlmUzyqMkmS3",
        "outputId": "40b4ec63-b364-4865-9816-6e6e662b5b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.18"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the Hyperspace Engine\n",
        "In the next step will Setting the environment requires the following steps\n",
        "\n",
        "\n",
        "1. Install the client API\n",
        "2. Connect to a server\n",
        "3. Create data schema file\n",
        "4. Create collection\n",
        "5. Ingest data\n",
        "6. Run query"
      ],
      "metadata": {
        "id": "s69wNfEeUW_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install the client API\n",
        "Hyperspace API can be installed directly from git, using the following command:"
      ],
      "metadata": {
        "id": "OO5o1hkQZwk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/hyper-space-io/hyperspace-py"
      ],
      "metadata": {
        "id": "7aS6oK7VWlPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Connect to a server\n",
        "\n",
        "Once the Hyperspace API is installed, the database can be accessed by creating a local instance of the Hyperspace client. This step requires host address, username and password."
      ],
      "metadata": {
        "id": "a922B_TJVkAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hyperspace\n",
        "\n",
        "hyperspace_client = hyperspace.HyperspaceClientApi(host='https://search-master-demo.development.hyper-space.xyz',\n",
        "                                                   username=username, password=password)\n"
      ],
      "metadata": {
        "id": "w_efZPFEVxuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before continuing, let us check that the cluster is live using the  \"**cluster_status()**\" command"
      ],
      "metadata": {
        "id": "jMBaZPVbjYsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperspace_client.cluster_status()"
      ],
      "metadata": {
        "id": "-lFMBR0zmKnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.   Create a Data Schema File\n",
        "\n",
        "Similarly to other search databases, Hyper-Space database requires a configuration file that outlines the data schema. Here, we create a config file that corresponds to the fields of the given dataset.\n",
        "\n",
        "For vector fields, we also provide the index type to be used, and the metric. . Current options for index include \"**brute_force**\", \"**hnsw**\", \"**ivf**\", and \"**bin_ivf**\" for binary vectors, and \"**IP**\" (inner product) as a metric for floating point vectors and \"**Hamming**\" ([hamming distance](https://en.wikipedia.org/wiki/Hamming_distance)) for binary vectors.\n",
        "\n",
        "Here, we use standrad inner product (metric = IP) with exact KNN (index = brute_force)."
      ],
      "metadata": {
        "id": "UU51tnzuWATM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "  \"configuration\": {\n",
        "    \"adult\": {\n",
        "      \"type\": \"boolean\"\n",
        "    },\n",
        "    \"belongs_to_collection\": {\n",
        "      \"type\": \"keyword\"\n",
        "    },\n",
        "    \"budget\": {\n",
        "      \"type\": \"integer\"\n",
        "    },\n",
        "    \"genres\": {\n",
        "      \"struct_type\": \"list\",\n",
        "      \"type\": \"keyword\"\n",
        "    },\n",
        "    \"id\": {\n",
        "      \"type\": \"integer\"\n",
        "    },\n",
        "    \"original_language\": {\n",
        "      \"type\": \"keyword\"\n",
        "    },\n",
        "    \"popularity\": {\n",
        "      \"type\": \"float\"\n",
        "    },\n",
        "    \"production_companies\": {\n",
        "      \"struct_type\": \"list\",\n",
        "      \"type\": \"keyword\"\n",
        "    },\n",
        "    \"production_countries\": {\n",
        "      \"struct_type\": \"list\",\n",
        "      \"type\": \"keyword\"\n",
        "    },\n",
        "    \"rating\": {\n",
        "      \"type\": \"float\"\n",
        "    },\n",
        "    \"release_date_unix_time\": {\n",
        "      \"type\": \"date\"\n",
        "    },\n",
        "    \"revenue\": {\n",
        "      \"type\": \"float\"\n",
        "    },\n",
        "    \"runtime_days\": {\n",
        "      \"type\": \"integer\"\n",
        "    },\n",
        "    \"spoken_languages\": {\n",
        "      \"struct_type\": \"list\",\n",
        "      \"type\": \"keyword\"\n",
        "    },\n",
        "    \"title\": {\n",
        "      \"type\": \"keyword\"\n",
        "    },\n",
        "     \"description embedding\": {\n",
        "            \"type\": \"dense_vector\",\n",
        "            \"dim\": 2048,\n",
        "            \"index_type\": \"brute_force\",\n",
        "            \"metric\": \"IP\"\n",
        "      }\n",
        "  }\n",
        "}\n",
        "\n",
        "with open('MovieRecommendation_config.json', 'w') as f:\n",
        "    f.write(json.dumps(config, indent=2))\n",
        "\n"
      ],
      "metadata": {
        "id": "3SKjACJmWJNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create Collection\n",
        "The Hyerspace engine stroes data in Collections, where each collecction commonly hosts data of similar context, etc. Each search is then perfomed within a collection. We create a collection using the command \"**create_collection**(schema_filename, collection_name)\"."
      ],
      "metadata": {
        "id": "NGhQomDXq8zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delete_collections = True\n",
        "collection_name = 'Movies'\n",
        "\n",
        "if delete_collections:\n",
        "  try:\n",
        "    hyperspace_client.delete_collection(collection_name)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "hyperspace_client.create_collection('MovieRecommendation_config.json', collection_name)\n",
        "hyperspace_client.cluster_status()"
      ],
      "metadata": {
        "id": "SJELyDwapZmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Ingest data\n",
        "\n",
        "In the next step we ingest the dataset in batches of 250 documents. This number can be controlled by user, and in particular, can be increased in order improve ingestion time. We add batches of data using the command **add_batch**(batch, collection_name)."
      ],
      "metadata": {
        "id": "SoAn8EuqqmPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "BATCH_SIZE = 500\n",
        "\n",
        "def chunker(df, size):\n",
        "    return (df.iloc[pos:pos + size] for pos in range(0, len(df), size))\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "i = 0\n",
        "\n",
        "for chunk in chunker(df.iloc[i:], BATCH_SIZE):\n",
        "    batch = [hyperspace.Document(str(i + j), row) for j, row in enumerate(chunk.to_dict('records'))]\n",
        "    i += BATCH_SIZE\n",
        "\n",
        "    if i % BATCH_SIZE == 0:\n",
        "        response = hyperspace_client.add_batch(batch, collection_name)\n",
        "        print(i, response)\n",
        "        batch.clear()\n",
        "\n",
        "\n",
        "hyperspace_client.commit('Movies')\n",
        "time = (time.time() - start)\n",
        "print(f\"ingestion took: {time} sec\")"
      ],
      "metadata": {
        "id": "LorxCA0DsrLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6218413-e312-4ceb-b341-5713f1fd2e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "1000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "1500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "2000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "2500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "3000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "3500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "4000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "4500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "5000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "5500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "6000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "6500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "7000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "7500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "8000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "8500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "9000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "9500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "10000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "10500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "11000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "11500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "12000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "12500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "13000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "13500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "14000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "14500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "15000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "15500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "16000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "16500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "17000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "17500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "18000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "18500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "19000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "19500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "20000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "20500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "21000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "21500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "22000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "22500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "23000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "23500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "24000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "24500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "25000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "25500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "26000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "26500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "27000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "27500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "28000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "28500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "29000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "29500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "30000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "30500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "31000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "31500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "32000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "32500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "33000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "33500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "34000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "34500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "35000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "35500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "36000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "36500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "37000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "37500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "38000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "38500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "39000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "39500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "40000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "40500 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "41000 {'code': 200, 'message': 'Batch successfully added', 'status': 'OK'}\n",
            "ingestion took: 121.37641835212708 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Running The Query\n",
        "\n",
        "### Creating The Query\n",
        "Hyperspace queries are created in python format and saved as strings. Classic search requires a pre-created score function."
      ],
      "metadata": {
        "id": "42F0n4dLs0sq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the score function\n",
        "The score function encorporates logic based on movied budget and rating, and gives bonus to movies of similar production_companies. Only movies of the same genre are returned. We will use two score functions. The first is used only for matching similar movies, while the other will perform agrregrations on the returned data."
      ],
      "metadata": {
        "id": "W5v7etLCFAo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sf_file = 'movies_score_function.py'\n",
        "hyperspace_client.set_function(sf_file, collection_name=collection_name, function_name='popular_films_recommendation')"
      ],
      "metadata": {
        "id": "z3trmq_WhggL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sf_agg_file = 'movie_aggregation_function.py'\n",
        "hyperspace_client.set_function(sf_agg_file, collection_name=collection_name, function_name='aggregation_films_recommendation')"
      ],
      "metadata": {
        "id": "Uagl1TobldFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is use the query logic and apply the query. We will run a vector search, followed by a hybrid search which includes analytic logic - boost based on rating, genres, etc. Let's start with vector search.\n",
        "\n",
        "### Vector Search\n",
        "We start with a simple search function, based on matching of embedded vectors using inner product and accurate KNN"
      ],
      "metadata": {
        "id": "_w_kr7POmgVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector = hyperspace_client.get_document(document_id='555', collection_name=collection_name)\n",
        "pprint({x: input_vector[x] for x in input_vector if x!= \"description embedding\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAkLsRCxlO9v",
        "outputId": "75596061-0bb9-4e30-9cf9-fbe9ddfc896c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'adult': False,\n",
            " 'belongs_to_collection': 'None',\n",
            " 'budget': 38000000,\n",
            " 'description text': 'young orphan boy James spill magic bag crocodile tongue '\n",
            "                     'find possession giant peach fly away strange '\n",
            "                     'landAdventures big grow tree',\n",
            " 'genres': \"['Adventure', 'Animation', 'Family']\",\n",
            " 'id': 10539,\n",
            " 'index': 651,\n",
            " 'level_0': 555,\n",
            " 'popularity': 13.217623,\n",
            " 'production_companies': \"['Walt Disney Pictures', 'Allied Filmmakers']\",\n",
            " 'production_countries': \"['US']\",\n",
            " 'rating': 5.692732773656594,\n",
            " 'revenue': 28921264.0,\n",
            " 'runtime_days': 79,\n",
            " 'spoken_languages': \"['en']\",\n",
            " 'title': 'James and the Giant Peach',\n",
            " 'unix_time': 829267200,\n",
            " 'year': 1996}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggrzx1WLtCZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ac4a54-7db0-4dfc-c4c4-5288ee1c1e6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "searching for matches for ' James and the Giant Peach '\n",
            "-------------------------------------------------\n",
            "Query run time = 99.1371ms\n",
            "-------------------------------------------------\n",
            "Rank  ID         Title                                    Score     \n",
            "=================================================================\n",
            "1     555        James and the Giant Peach                1.01      \n",
            "2     36676      Air Mater                                0.71      \n",
            "3     20863      A Kid for Two Farthings                  0.7       \n",
            "4     40550      The Famous Box Trick                     0.7       \n",
            "5     15758      Mystics in Bali                          0.67      \n",
            "6     23548      Magic Boy                                0.66      \n",
            "7     9331       Freedomland                              0.61      \n",
            "8     454        Sleepless in Seattle                     0.61      \n",
            "9     19950      Gamera vs. Viras                         0.6       \n",
            "10    16095      Magic Christmas Tree                     0.58      \n"
          ]
        }
      ],
      "source": [
        "query = {\n",
        "    'params': input_vector,\n",
        "    \"knn\": {\n",
        "        \"query\": {\"boost\": 0}, # boost = 0 , means no metadata filtering\n",
        "        \"description embedding\": {\n",
        "            \"boost\": 1,\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"searching for matches for '\",input_vector[\"title\"],\"'\")\n",
        "print(\"-------------------------------------------------\")\n",
        "results = hyperspace_client.search(query,\n",
        "                                        size=10,\n",
        "                                        function_name='popular_films_recommendation',\n",
        "                                        collection_name=collection_name)\n",
        "\n",
        "candidates = results['candidates']\n",
        "\n",
        "print(f\"Query run time = {results['took_ms']}ms\")\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "\n",
        "print(\"{:<5} {:<10} {:<40} {:<10}\".format(\"Rank\", \"ID\", \"Title\", \"Score\"))\n",
        "print(\"=\"*65)\n",
        "\n",
        "for i, result in enumerate(results['similarity']):\n",
        "    api_response = hyperspace_client.get_document(document_id=result['document_id'], collection_name=collection_name)\n",
        "    print(\"{:<5} {:<10} {:<40} {:<10}\".format(i + 1, result['document_id'], api_response['title'][:40], round(result['score'], 2)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classic Search\n",
        "Let us use thescore function, and run classic search (keyword matching)."
      ],
      "metadata": {
        "id": "JyXvp9KwocLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector = hyperspace_client.get_document(document_id='555', collection_name=collection_name)\n",
        "\n",
        "print(\"searching for matches for '\",input_vector[\"title\"],\"'\")\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "query = {\n",
        "    'params': input_vector,\n",
        "    \"knn\": {\n",
        "        \"query\": {\"boost\": 2}, # boost = 0 , means no metadata filtering\n",
        "    }\n",
        "}\n",
        "\n",
        "results = hyperspace_client.search(query,\n",
        "                                        size=10,\n",
        "                                        function_name='popular_films_recommendation',\n",
        "                                        collection_name=collection_name)\n",
        "\n",
        "candidates = results['candidates']\n",
        "\n",
        "print(f\"Query run time = {results['took_ms']}ms\")\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "\n",
        "print(\"{:<5} {:<10} {:<40} {:<10}\".format(\"Rank\", \"ID\", \"Title\", \"Score\"))\n",
        "print(\"=\"*65)\n",
        "\n",
        "for i, result in enumerate(results['similarity']):\n",
        "    api_response = hyperspace_client.get_document(document_id=result['document_id'], collection_name=collection_name)\n",
        "    print(\"{:<5} {:<10} {:<40} {:<10}\".format(i + 1, result['document_id'], api_response['title'][:40], round(result['score'], 2)))\n",
        "\n"
      ],
      "metadata": {
        "id": "2ihn-UwxoR9E",
        "outputId": "11dd978a-da83-4120-c82b-4ff08b666e34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "searching for matches for ' James and the Giant Peach '\n",
            "-------------------------------------------------\n",
            "Query run time = 3.81724ms\n",
            "------------------------------------------------------\n",
            "Rank  ID         Title                                    Score     \n",
            "=================================================================\n",
            "1     37206      Moana                                    360.14    \n",
            "2     27368      The Good Dinosaur                        280.32    \n",
            "3     11584      The Tale of Despereaux                   280.24    \n",
            "4     14703      Mars Needs Moms                          280.14    \n",
            "5     36753      Trolls                                   280.14    \n",
            "6     30255      Phantom Boy                              240.6     \n",
            "7     36977      Little Longnose                          240.37    \n",
            "8     16752      Jack-Jack Attack                         240.14    \n",
            "9     21511      No Time for Nuts                         240.14    \n",
            "10    22010      The Little Fox                           240.14    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hybrid Search\n",
        "In the next step we apply Hybrid search, which combines classic and vector search."
      ],
      "metadata": {
        "id": "-8a2utcQTavM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector = hyperspace_client.get_document(document_id='555', collection_name=collection_name)\n",
        "\n",
        "print(\"searching for matches for '\",input_vector[\"title\"],\"'\")\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "query = {\n",
        "    'params': input_vector,\n",
        "    \"knn\": {\n",
        "        \"query\": {\"boost\": 1},\n",
        "        \"description embedding\": {\n",
        "            \"boost\": 200,\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "results = hyperspace_client.search(query,\n",
        "                                        size=10,\n",
        "                                        function_name='aggregation_films_recommendation',\n",
        "                                        collection_name=collection_name)\n",
        "\n",
        "candidates = results['candidates']\n",
        "\n",
        "print(f\"Query run time = {results['took_ms']}ms\")\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "\n",
        "print(\"{:<5} {:<10} {:<40} {:<10}\".format(\"Rank\", \"ID\", \"Title\", \"Score\"))\n",
        "print(\"=\"*65)\n",
        "\n",
        "for i, result in enumerate(results['similarity']):\n",
        "    api_response = hyperspace_client.get_document(document_id=result['document_id'], collection_name=collection_name)\n",
        "    print(\"{:<5} {:<10} {:<40} {:<10}\".format(i + 1, result['document_id'], api_response['title'][:40], round(result['score'], 2)))"
      ],
      "metadata": {
        "id": "WyNbh77OTeJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0afb6c4-7256-48ef-c967-9673c19549a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "searching for matches for ' James and the Giant Peach '\n",
            "-------------------------------------------------\n",
            "Query run time = 4.24484ms\n",
            "------------------------------------------------------\n",
            "Rank  ID         Title                                    Score     \n",
            "=================================================================\n",
            "1     30255      Phantom Boy                              210.87    \n",
            "2     37206      Moana                                    180.07    \n",
            "3     27368      The Good Dinosaur                        176.07    \n",
            "4     36977      Little Longnose                          164.59    \n",
            "5     11584      The Tale of Despereaux                   159.87    \n",
            "6     14703      Mars Needs Moms                          140.07    \n",
            "7     36753      Trolls                                   140.07    \n",
            "8     16752      Jack-Jack Attack                         120.07    \n",
            "9     21511      No Time for Nuts                         120.07    \n",
            "10    22010      The Little Fox                           120.07    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Search and Agggregations\n",
        "In the final part, we will use a different score function to aggregatre various fields of the data. The aggregation is perfomed in the score function, where each aggregation is perfomed over all candidates that passed the filtering up to the step that hosts it."
      ],
      "metadata": {
        "id": "IvPKgC2HLINB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector = hyperspace_client.get_document(document_id='555', collection_name=collection_name)\n",
        "\n",
        "print(\"searching for matches for '\",input_vector[\"title\"],\"'\")\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "query = {\n",
        "    'params': input_vector,\n",
        "    \"knn\": {\n",
        "        \"query\": {\"boost\": 1},\n",
        "        \"description embedding\": {\n",
        "            \"boost\": 200,\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "results = hyperspace_client.search(query,\n",
        "                                        size=10,\n",
        "                                        function_name='aggregation_films_recommendation',\n",
        "                                        collection_name=collection_name)\n",
        "\n",
        "candidates = results['candidates']\n",
        "\n",
        "print(f\"Query run time = {results['took_ms']}ms\")\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "\n",
        "print(\"{:<5} {:<10} {:<40} {:<10}\".format(\"Rank\", \"ID\", \"Title\", \"Score\"))\n",
        "print(\"=\"*65)\n",
        "\n",
        "for i, result in enumerate(results['similarity']):\n",
        "    api_response = hyperspace_client.get_document(document_id=result['document_id'], collection_name=collection_name)\n",
        "    print(\"{:<5} {:<10} {:<40} {:<10}\".format(i + 1, result['document_id'], api_response['title'][:40], round(result['score'], 2)))\n",
        "\n",
        "\n",
        "print(\"\\n----------------- Aggregations --------------------\")\n",
        "print(\"{:<50} {:<15}\".format(\"Aggregation\", \"Value\"))\n",
        "print(\"=\"*55)\n",
        "\n",
        "for i, agg in enumerate(results['aggregations']):\n",
        "    if results['aggregations'][agg]['agg_type'] == \"count\":\n",
        "        print(\"{:<50} {:<15}\".format(agg, results['aggregations'][agg][\"count\"]))\n",
        "    else:\n",
        "        print(\"{:<50} {:<15}\".format(agg, round(results['aggregations'][agg][\"value\"],2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpF6vpcelzsJ",
        "outputId": "df54ed9a-cf02-4fe4-988f-b8e6593177c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "searching for matches for ' James and the Giant Peach '\n",
            "-------------------------------------------------\n",
            "Query run time = 5.30331ms\n",
            "------------------------------------------------------\n",
            "Rank  ID         Title                                    Score     \n",
            "=================================================================\n",
            "1     30255      Phantom Boy                              210.87    \n",
            "2     37206      Moana                                    180.07    \n",
            "3     27368      The Good Dinosaur                        176.07    \n",
            "4     36977      Little Longnose                          164.59    \n",
            "5     11584      The Tale of Despereaux                   159.87    \n",
            "6     14703      Mars Needs Moms                          140.07    \n",
            "7     36753      Trolls                                   140.07    \n",
            "8     16752      Jack-Jack Attack                         120.07    \n",
            "9     21511      No Time for Nuts                         120.07    \n",
            "10    22010      The Little Fox                           120.07    \n",
            "\n",
            "----------------- Aggregations --------------------\n",
            "Aggregation                                        Value          \n",
            "=======================================================\n",
            "avg revenue for movies with runtime_days > 200d    0.0            \n",
            "avg revenue for movies with budget >= 1m           201125504.0    \n",
            "total avg_rating                                   5.54           \n",
            "total max_rating                                   7.07           \n",
            "total max_popularity                               16.84          \n",
            "avg revenue for rating >= 7.0                      643034496.0    \n",
            "total min_rating                                   5.24           \n",
            "total avg_popularity                               5.28           \n",
            "count movies with rating >= 7.0                    1              \n"
          ]
        }
      ]
    },
{
      "cell_type": "markdown",
      "source": [
        "![63f78014766fd30436c18a79_Hyperspace - navbar logo.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "_trhSpIUhamm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fo0Q5qmIWijs",
        "SoAn8EuqqmPW"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
